{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os,json\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pk\n",
    "min_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36674it [00:33, 1100.61it/s]\n"
     ]
    }
   ],
   "source": [
    "db = pd.read_csv('../data/news/News_result.csv')\n",
    "chars = {}\n",
    "#chars = dict()\n",
    "for indexs, a in tqdm(db.iterrows()):\n",
    "    #print(a['content'])\n",
    "    for w in a['content']: # 纯文本，不用分词\n",
    "        chars[w] = chars.get(w,0) + 1\n",
    "    for w in a['title']: # 纯文本，不用分词\n",
    "        chars[w] = chars.get(w,0) + 1\n",
    "chars = {i:j for i,j in chars.items() if j >= min_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2char = {i+4:j for i,j in enumerate(chars)}\n",
    "char2id = {j:i for i,j in id2char.items()}\n",
    "min_count = 32\n",
    "maxlen = 400\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "char_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2id(s, start_end=False):\n",
    "    # 文字转整数id\n",
    "    if start_end: # 补上<start>和<end>标记， ‘2’代表start, '3'end，‘1’生僻字\n",
    "        ids = [char2id.get(c, 1) for c in s[:maxlen-2]]\n",
    "        ids = [2] + ids + [3]\n",
    "    else: # 普通转化\n",
    "        ids = [char2id.get(c, 1) for c in s[:maxlen]]\n",
    "    return ids\n",
    "\n",
    "def id2str(ids):\n",
    "    # id转文字，找不到的用空字符代替\n",
    "    return ''.join([id2char.get(i, '') for i in ids])\n",
    "\n",
    "\n",
    "def padding(x):\n",
    "    # padding至batch内的最大长度\n",
    "    ml = max([len(i) for i in x])\n",
    "    return [i + [0] * (ml-len(i)) for i in x]\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    # 数据生成器\n",
    "    X,Y = [],[]\n",
    "    while True:\n",
    "        for indexs, a in db.iterrows():\n",
    "            X.append(str2id(a['content']))\n",
    "            Y.append(str2id(a['title'], start_end=True))\n",
    "            if len(X) == batch_size:\n",
    "                X = np.array(padding(X))\n",
    "                Y = np.array(padding(Y))\n",
    "                yield [X,Y], None\n",
    "                X,Y = [],[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(x_set): # 输出一个词表大小的向量，来标记该词是否在文章出现过\n",
    "    x, x_mask = x_set\n",
    "    x = K.cast(x, 'int32')\n",
    "    x = K.one_hot(x, len(chars)+4)\n",
    "    x = K.sum(x_mask * x, 1, keepdims=True)\n",
    "    x = K.cast(K.greater(x, 0.5), 'float32')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleShift(Layer):\n",
    "    \"\"\"缩放平移变换层（Scale and shift）\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaleShift, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        kernel_shape = (1,)*(len(input_shape)-1) + (input_shape[-1],)\n",
    "        self.log_scale = self.add_weight(name='log_scale',\n",
    "                                         shape=kernel_shape,\n",
    "                                         initializer='zeros')\n",
    "        self.shift = self.add_weight(name='shift',\n",
    "                                     shape=kernel_shape,\n",
    "                                     initializer='zeros')\n",
    "    def call(self, inputs):\n",
    "        x_outs = K.exp(self.log_scale) * inputs + self.shift\n",
    "        return x_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_size = 128\n",
    "x_in = Input(shape=(None,))\n",
    "y_in = Input(shape=(None,))\n",
    "x = x_in\n",
    "y = y_in\n",
    "x_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(x)\n",
    "y_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(y)\n",
    "\n",
    "x_one_hot = Lambda(to_one_hot)([x, x_mask])\n",
    "x_prior = ScaleShift()(x_one_hot) # 学习输出的先验分布（标题的字词很可能在文章出现过）\n",
    "\n",
    "embedding = Embedding(len(chars)+4, char_size)\n",
    "x = embedding(x)\n",
    "y = embedding(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder，双层双向LSTM\n",
    "x = Bidirectional(LSTM(int(char_size/2), return_sequences=True))(x)\n",
    "x = Bidirectional(LSTM(int(char_size/2), return_sequences=True))(x)\n",
    "\n",
    "# decoder，双层单向LSTM\n",
    "y = LSTM(char_size, return_sequences=True)(y)\n",
    "y = LSTM(char_size, return_sequences=True)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interact(Layer):\n",
    "    \"\"\"交互层，负责融合encoder和decoder的信息\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Interact, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        in_dim = input_shape[0][-1]\n",
    "        out_dim = input_shape[1][-1]\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(in_dim, out_dim),\n",
    "                                      initializer='glorot_normal')\n",
    "    def call(self, inputs):\n",
    "        q, v, v_mask = inputs\n",
    "        k = v\n",
    "        mv = K.max(v - (1. - v_mask) * 1e10, axis=1, keepdims=True) # maxpooling1d\n",
    "        mv = mv + K.zeros_like(q[:,:,:1]) # 将mv重复至“q的timesteps”份\n",
    "        # 下面几步只是实现了一个乘性attention\n",
    "        qw = K.dot(q, self.kernel)\n",
    "        a = K.batch_dot(qw, k, [2, 2]) / 10.\n",
    "        a -= (1. - K.permute_dimensions(v_mask, [0, 2, 1])) * 1e10\n",
    "        a = K.softmax(a)\n",
    "        o = K.batch_dot(a, v, [2, 1])\n",
    "        # 将各步结果拼接\n",
    "        return K.concatenate([o, q, mv], 2)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, input_shape[0][1],\n",
    "                input_shape[0][2]+input_shape[1][2]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = Interact()([y, x, x_mask])\n",
    "xy = Dense(512, activation='relu')(xy)\n",
    "xy = Dense(len(chars)+4)(xy)\n",
    "xy = Lambda(lambda x: (x[0]+x[1])/2)([xy, x_prior]) # 与先验结果平均\n",
    "xy = Activation('softmax')(xy)\n",
    "\n",
    "# 交叉熵作为loss，但mask掉padding部分\n",
    "cross_entropy = K.sparse_categorical_crossentropy(y_in[:, 1:], xy[:, :-1])\n",
    "loss = K.sum(cross_entropy * y_mask[:, 1:, 0]) / K.sum(y_mask[:, 1:, 0])\n",
    "\n",
    "model = Model([x_in, y_in], xy)\n",
    "model.add_loss(loss)\n",
    "model.compile(optimizer=Adam(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_title(s, topk=3):\n",
    "    \"\"\"beam search解码\n",
    "    每次只保留topk个最优候选结果；如果topk=1，那么就是贪心搜索\n",
    "    \"\"\"\n",
    "    xid = np.array([str2id(s)] * topk) # 输入转id\n",
    "    yid = np.array([[2]] * topk) # 解码均以<start>开通，这里<start>的id为2\n",
    "    scores = [0] * topk # 候选答案分数\n",
    "    for i in range(50): # 强制要求标题不超过50字\n",
    "        proba = model.predict([xid, yid])[:, i, 3:] # 直接忽略<padding>、<unk>、<start>\n",
    "        log_proba = np.log(proba + 1e-6) # 取对数，方便计算\n",
    "        arg_topk = log_proba.argsort(axis=1)[:,-topk:] # 每一项选出topk\n",
    "        _yid = [] # 暂存的候选目标序列\n",
    "        _scores = [] # 暂存的候选目标序列得分\n",
    "        if i == 0:\n",
    "            for j in range(topk):\n",
    "                _yid.append(list(yid[j]) + [arg_topk[0][j]+3])\n",
    "                _scores.append(scores[j] + log_proba[0][arg_topk[0][j]])\n",
    "        else:\n",
    "            for j in range(len(xid)):\n",
    "                for k in range(topk): # 遍历topk*topk的组合\n",
    "                    _yid.append(list(yid[j]) + [arg_topk[j][k]+3])\n",
    "                    _scores.append(scores[j] + log_proba[j][arg_topk[j][k]])\n",
    "            _arg_topk = np.argsort(_scores)[-topk:] # 从中选出新的topk\n",
    "            _yid = [_yid[k] for k in _arg_topk]\n",
    "            _scores = [_scores[k] for k in _arg_topk]\n",
    "        yid = []\n",
    "        scores = []\n",
    "        for k in range(len(xid)):\n",
    "            if _yid[k][-1] == 3: # 找到<end>就返回\n",
    "                return id2str(_yid[k])\n",
    "            else:\n",
    "                yid.append(_yid[k])\n",
    "                scores.append(_scores[k])\n",
    "        yid = np.array(yid)\n",
    "    # 如果50字都找不到<end>，直接返回\n",
    "    return id2str(yid[np.argmax(scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = u'夏天来临，皮肤在强烈紫外线的照射下，晒伤不可避免，因此，晒后及时修复显得尤为重要，否则可能会造成长期伤害。专家表示，选择晒后护肤品要慎重，芦荟凝胶是最安全，有效的一种选择，晒伤严重者，还请及时就医。'\n",
    "s2 = u'8月28日，网络爆料称，华住集团旗下连锁酒店用户数据疑似发生泄露。从卖家发布的内容看，数据包含华住旗下汉庭、禧玥、桔子、宜必思等10余个品牌酒店的住客信息。泄露的信息包括华住官网注册资料、酒店入住登记的身份信息及酒店开房记录，住客姓名、手机号、邮箱、身份证号、登录账号密码等。卖家对这个约5亿条数据打包出售。第三方安全平台威胁猎人对信息出售者提供的三万条数据进行验证，认为数据真实性非常高。当天下午，华住集团发声明称，已在内部迅速开展核查，并第一时间报警。当晚，上海警方消息称，接到华住集团报案，警方已经介入调查。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(Callback):\n",
    "    def __init__(self):\n",
    "        self.lowest = 1e10\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 训练过程中观察一两个例子，显示标题质量提高的过程\n",
    "        print(gen_title(s1))\n",
    "        print(gen_title(s2))\n",
    "        # 保存最优结果\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save_weights('./best_model.weights')\n",
    "evaluator = Evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22222222222222222222222222222222222222222222222222'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_title(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[60,1] = 2395 is not in [0, 62)\n\t [[Node: embedding_1_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1_1/Cast, training/Adam/gradients/embedding_1_1/embedding_lookup_grad/concat/axis)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-ba63e1eb3cba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m callbacks=[evaluator])\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: indices[60,1] = 2395 is not in [0, 62)\n\t [[Node: embedding_1_1/embedding_lookup = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training/Adam/Assign_2\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_1/embeddings/read, embedding_1_1/Cast, training/Adam/gradients/embedding_1_1/embedding_lookup_grad/concat/axis)]]"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "model.fit_generator(data_generator(),\n",
    "                    steps_per_epoch=3,\n",
    "                    epochs=epochs,\n",
    "callbacks=[evaluator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-9cd6dc188a18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchar2id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "char2id(gen_title(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 3\n",
    "xid = np.array([str2id(s1)] * topk) # 输入转id\n",
    "yid = np.array([[2]] * topk) # 解码均以<start>开通，这里<start>的id为2\n",
    "scores = [0] * topk # 候选答案分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict([xid, yid])[:, 0, 3:]\n",
    "log_proba = np.log(proba + 1e-6)\n",
    "arg_topk = log_proba.argsort(axis=1)[:,-topk:] # 每一项选出topk\n",
    "_yid = [] # 暂存的候选目标序列\n",
    "_scores = [] # 暂存的候选目标序列得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,  1,  1, 39,  1,  1,\n",
       "         1, 17,  1,  1,  1,  1,  1,  1, 17,  1,  1, 17,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1, 29,  1,  1, 17,  1,  1,  1,  1, 50,  1,  1,\n",
       "         1,  1,  1,  1, 59,  1,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,\n",
       "        16,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,  1, 25, 17, 15,  1,\n",
       "        39,  1,  1,  1,  1, 17,  1,  1,  1,  1,  1, 17,  1,  1,  1,  1,\n",
       "         1,  1, 59],\n",
       "       [ 1,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,  1,  1, 39,  1,  1,\n",
       "         1, 17,  1,  1,  1,  1,  1,  1, 17,  1,  1, 17,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1, 29,  1,  1, 17,  1,  1,  1,  1, 50,  1,  1,\n",
       "         1,  1,  1,  1, 59,  1,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,\n",
       "        16,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,  1, 25, 17, 15,  1,\n",
       "        39,  1,  1,  1,  1, 17,  1,  1,  1,  1,  1, 17,  1,  1,  1,  1,\n",
       "         1,  1, 59],\n",
       "       [ 1,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,  1,  1, 39,  1,  1,\n",
       "         1, 17,  1,  1,  1,  1,  1,  1, 17,  1,  1, 17,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1, 29,  1,  1, 17,  1,  1,  1,  1, 50,  1,  1,\n",
       "         1,  1,  1,  1, 59,  1,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,\n",
       "        16,  1,  1,  1, 17,  1,  1,  1,  1,  1,  1,  1, 25, 17, 15,  1,\n",
       "        39,  1,  1,  1,  1, 17,  1,  1,  1,  1,  1, 17,  1,  1,  1,  1,\n",
       "         1,  1, 59]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = u'2018年7月13日，证监会召开例行发布会，以下为发布会主要内容：\\\\n证监会集中查处9家未按时披露年报的上市公司\\\\n7月13日，中国证监会新闻发言人高莉表示，日前证监会部署了2018年第三批案件的专项执法行动，集中查处了未按时披露年报的9家上市公司。下一步，证监会将严肃查处相关行为，督促上市公司及时真实完整地披露年度报告。\\\\n证监会对6宗案件作出行政处罚 涉及尔康制药等\\\\n证监会对6宗案件做出行政处罚，包括1宗操纵市场案，2宗内幕交易案，1宗期货公司违反风险监管指标案和2宗信息披露违法违规案。涉及珠海中富、尔康制药等公司。\\\\n证监会强调，将对信息披露违法行为始终保持高压态势，一旦发现必将果断亮剑，重拳出击，坚决查处信息披露违法行为并依法追究相关人责任。\\\\n证券公司被司法机关侦查，将不予受理或中止审查申请材料\\\\n证监会新闻发言人高莉表示，近日证监会发布了关于修改证监会行政许可实施程序规定的决定。主要修订内容是，证券公司、证券服务机构及有关人员因涉嫌违法违规被证监会及其派出机构立案调查，或者被司法机关侦查，尚未结案，且涉案行为与其为申请人提供服务的行为属于同类业务的，证监会将不予受理或者中止审查相关材料。\\\\n（内容根据记者现场速记整理）'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1794"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id.get(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
